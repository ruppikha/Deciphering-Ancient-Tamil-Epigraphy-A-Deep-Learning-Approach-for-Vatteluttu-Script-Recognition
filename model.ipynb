{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7213979,"sourceType":"datasetVersion","datasetId":4174475}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pdZ\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport os\nimport random \nimport cv2\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelBinarizer\nfrom tensorflow.keras.utils import to_categorical  # Updated ixmport\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import optimizers\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras import backend as K\nfrom keras.layers import Dense, Activation, Flatten, Dense,MaxPooling2D, Dropout\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T08:19:39.613430Z","iopub.execute_input":"2024-10-06T08:19:39.613841Z","iopub.status.idle":"2024-10-06T08:19:50.175185Z","shell.execute_reply.started":"2024-10-06T08:19:39.613803Z","shell.execute_reply":"2024-10-06T08:19:50.174178Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\n\nmodern_dir = '/kaggle/input/8th-century-tamil-inscriptions/Modern characters'\n\ndef custom_preprocessing(image):\n    # Resize image to (32, 32)\n    image = cv2.resize(image, (32, 32))\n\n    # Ensure the image is in the correct format (uint8)\n    if image.dtype != np.uint8:\n        image = (image * 255).astype(np.uint8)\n\n    # Apply Gaussian smoothing\n    smoothed_image = cv2.GaussianBlur(image, (5, 5), 0)\n\n    # Apply adaptive thresholding\n    adaptive_thresholded = cv2.adaptiveThreshold(\n        smoothed_image,\n        255,\n        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n        cv2.THRESH_BINARY,\n        11,\n        2\n    )\n\n    # Normalize the pixel values to the range [0, 1]\n    normalized_image = adaptive_thresholded / 255.0\n\n    return normalized_image\n\n\n# Create ImageDataGenerator for both categorized and augmented images\ndatagen = ImageDataGenerator(\n    rescale=1./255,  # Normalize pixel values to [0, 1]\n    validation_split=0.2  # Split 80% for training, 20% for validation\n)\n\n# Custom generator to apply preprocessing\ndef custom_generator(directory, datagen):\n    for batch in datagen.flow_from_directory(directory,\n                                             target_size=(32, 32),\n                                             color_mode='grayscale',  # Keep this as is since images are grayscale\n                                             batch_size=32,\n                                             class_mode='sparse',\n                                             shuffle=True,\n                                             subset='training'):\n        # Ensure the batch contains images and preprocess them\n        processed_batch = np.array([custom_preprocessing(image) for image in batch[0]])\n        yield processed_batch, batch[1]\n\naugmented_generator = custom_generator('/kaggle/input/8th-century-tamil-inscriptions/augmented_images', datagen)\ncategorised_generator = custom_generator('/kaggle/input/8th-century-tamil-inscriptions/images_categorised', datagen)\n\n# Custom generator to apply preprocessing\ndef custom_generator(directory, datagen):\n    for batch in datagen.flow_from_directory(directory,\n                                             target_size=(32, 32),\n                                             color_mode='grayscale',\n                                             batch_size=32,\n                                             class_mode='sparse',\n                                             shuffle=True,\n                                             subset='training'):\n        processed_batch = np.array([custom_preprocessing(image) for image in batch[0]])\n        yield processed_batch, batch[1]\n\n        \n\ndef modern_generator_func(directory, datagen):\n    for batch in datagen.flow_from_directory(directory, \n                                             target_size=(32, 32), \n                                             color_mode='grayscale', \n                                             batch_size=32, \n                                             class_mode='sparse', \n                                             shuffle=False):\n        processed_batch = np.array([custom_preprocessing(image) for image in batch[0]])\n        yield processed_batch, batch[1]\n\nmodern_generator = modern_generator_func(modern_dir, datagen)\n\n\n# Function to load data from a generator\ndef load_data(generator, num_batches=10):\n    images, labels = [], []\n    for _ in range(num_batches):\n        batch = next(generator)\n        images.append(batch[0])\n        labels.append(batch[1])\n    return np.concatenate(images), np.concatenate(labels)\n\n# Load the augmented and categorised data\naugmented_images, augmented_labels = load_data(augmented_generator)\ncategorised_images, categorised_labels = load_data(categorised_generator)\n\n# Combine the images and labels from augmented and categorised folders\ncombined_images = np.concatenate([augmented_images, categorised_images], axis=0)\ncombined_labels = np.concatenate([augmented_labels, categorised_labels], axis=0)\nmodern_images, modern_labels = load_data(modern_generator)\n\n\n# Split the combined dataset into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(\n    combined_images, combined_labels, test_size=0.2, random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T08:20:17.951174Z","iopub.execute_input":"2024-10-06T08:20:17.952127Z","iopub.status.idle":"2024-10-06T08:20:21.230484Z","shell.execute_reply.started":"2024-10-06T08:20:17.952079Z","shell.execute_reply":"2024-10-06T08:20:21.229383Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 1369 images belonging to 27 classes.\nFound 136 images belonging to 27 classes.\nFound 27 images belonging to 27 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Create pairs of images for the Siamese network\ndef create_pairs(ancient_images, modern_images, labels):\n    pairs_ancient = []\n    pairs_modern = []\n    pair_labels = []\n    \n    for i in range(len(labels)):\n        for j in range(len(modern_labels)):\n            pairs_ancient.append(ancient_images[i])\n            pairs_modern.append(modern_images[j])\n            pair_labels.append(1 if labels[i] == modern_labels[j] else 0)\n    \n    return np.array(pairs_ancient), np.array(pairs_modern), np.array(pair_labels)\n\n# Create the pairs\nX_ancient, X_modern, y_pairs = create_pairs(combined_images, modern_images, combined_labels)\n\n# Split into train and test sets\nX_ancient_train, X_ancient_test, X_modern_train, X_modern_test, y_train, y_test = train_test_split(\n    X_ancient, X_modern, y_pairs, test_size=0.2, random_state=42\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T08:20:22.114412Z","iopub.execute_input":"2024-10-06T08:20:22.114885Z","iopub.status.idle":"2024-10-06T08:20:27.629547Z","shell.execute_reply.started":"2024-10-06T08:20:22.114842Z","shell.execute_reply":"2024-10-06T08:20:27.628521Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, MaxPooling2D, Conv2D, Lambda, Reshape, LSTM, Bidirectional\nimport tensorflow as tf  # Ensure TensorFlow is imported\n\n# Define the base network\ndef create_base_network(input_shape):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding=\"same\", activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(128, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.2))\n    return model\n\n# Define the Siamese network\ndef create_siamese_network(input_shape=(32, 32, 1), num_classes=28):\n    base_network = create_base_network(input_shape)\n    \n    input_ancient = Input(shape=input_shape)\n    input_modern = Input(shape=input_shape)\n    \n    processed_ancient = base_network(input_ancient)\n    processed_modern = base_network(input_modern)\n    \n    merged_vector = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([processed_ancient, processed_modern])\n    \n    # Reshape for RNN\n    reshaped_vector = Reshape((4, 32))(merged_vector)  # Assuming a sequence length of 4 and feature size of 32\n\n    # Bidirectional LSTM layers\n    lstm_output = Bidirectional(LSTM(64, return_sequences=False, dropout=0.25))(reshaped_vector)\n    \n    output = Dense(num_classes, activation='softmax')(lstm_output)\n    \n    siamese_model = Model(inputs=[input_ancient, input_modern], outputs=output)\n    \n    return siamese_model\n\n# Create the Siamese model\nsiamese_model = create_siamese_network()\n\n# Display the model summary\nsiamese_model.summary()\n\n# Compile the model\nsiamese_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T08:20:27.631654Z","iopub.execute_input":"2024-10-06T08:20:27.632051Z","iopub.status.idle":"2024-10-06T08:20:28.179901Z","shell.execute_reply.started":"2024-10-06T08:20:27.632010Z","shell.execute_reply":"2024-10-06T08:20:28.178705Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_11\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m158,336\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n│                     │                   │            │ sequential[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m49,664\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)        │      \u001b[38;5;34m3,612\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">158,336</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n│                     │                   │            │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,612</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m211,612\u001b[0m (826.61 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">211,612</span> (826.61 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m211,612\u001b[0m (826.61 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">211,612</span> (826.61 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"\n# Compile the model for multi-class classification\nsiamese_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Convert labels to one-hot encoding for multi-class classification\ny_train = to_categorical(y_train, num_classes=28)\ny_test = to_categorical(y_test, num_classes=28)\ny_train = y_train.reshape(-1, 28)\ny_test = y_test.reshape(-1, 28)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T08:20:36.609135Z","iopub.execute_input":"2024-10-06T08:20:36.609632Z","iopub.status.idle":"2024-10-06T08:20:36.645391Z","shell.execute_reply.started":"2024-10-06T08:20:36.609584Z","shell.execute_reply":"2024-10-06T08:20:36.644120Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping \nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=2,\n    restore_best_weights=True\n)\n\n# Train the model with EarlyStopping\nhistory = siamese_model.fit(\n    [X_ancient_train, X_modern_train],\n    y_train,\n    validation_data=([X_ancient_test, X_modern_test], y_test),\n    epochs=20,\n    batch_size=32,\n    callbacks=[early_stopping]\n)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T08:20:38.006570Z","iopub.execute_input":"2024-10-06T08:20:38.007025Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m3996/3996\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 52ms/step - accuracy: 0.9609 - loss: 0.1963 - val_accuracy: 0.9540 - val_loss: 0.1172\nEpoch 2/20\n\u001b[1m3623/3996\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m17s\u001b[0m 47ms/step - accuracy: 0.9711 - loss: 0.0953","output_type":"stream"}]},{"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}